defaults:
  - encoder: hf_bert

# A trained bi-encoder checkpoint file to initialize the model
model_file: /mnt/sdd/shpark/cos/models/cos_nq_ott_hotpot_finetuned_6_experts.ckpt


# which (ctx or query) encoder to be used for embedding generation
encoder_type: ctx

# output .tsv file path to write results to
out_file: /mnt/sdd/shpark/graph/embeds/edge_embeds_author_2
expert_id: 1
mean_pool: False
# Whether to lower case the input text. Set True for uncased models, False for the cased ones.
do_lower_case: True
encoder_path:
# Number(0-based) of data shard to process
shard_id: 0

# Total amount of data shards
num_shards: 1

gpu_id: -1
num_gpus: 3

# Batch size for the passage encoder forward pass (works in DataParallel mode)
batch_size: 1536

tables_as_passages: False

# tokens which won't be slit by tokenizer
special_tokens:

tables_chunk_sz: 100

# TODO
tables_split_type: type1


# TODO: move to a conf group
# local_rank for distributed training on gpus
local_rank: -1
device:
distributed_world_size:
distributed_port:
no_cuda: False
n_gpu:
fp16: False

# For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#        "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: O1

# Input path
preprocessed_graph_path: /mnt/sdd/shpark/graph/preprocess/preprocess_table_graph_author_w_score_2.json
graph_path: /mnt/sdd/shpark/graph/entity_linking_results/table_graph_author_w_score.json
graph_collection_name: table_chunks_to_passages_shard_author
table_mention_collection_name: all_table_chunks_span_prediction_original
table_collection_name: ott_table
passage_collection_name: ott_wiki_passages
# Hierarchical level
hierarchical_level: 'middle' # edge, star, middle
max_num_edges: 2
top_k_passages: 1

# MongoDB
dbname: mydatabase
username: root
password: 1234
port: 27017